{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d553d947-1083-461b-91d2-105b65b71bff",
   "metadata": {},
   "source": [
    "# Debug Bert notebook\n",
    "- [x] trivial case, with predicting shifted tokens, instead of mlm.\n",
    "- [x] trivial case, of classifying presence of marker token, at a random position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d79629-e31f-483b-8001-9e8e3362dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTango\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tokentango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe96db1-833e-4ed7-addc-ba792134b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch import nn, functional as F\n",
    "from plotly import express as px\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.nn.utils as nn_utils\n",
    "import os\n",
    "import uuid\n",
    "#from muon import MuonWithAuxAdam\n",
    "#import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994bfb1a-207e-41c1-b826-c84e6db90645",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795d072a-db2e-4197-90de-7fb5242be9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4948d-e356-4e72-ae79-837b6db13dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embeddings padding_idx = 0, might conflict with cls, or other tokens? (model.py)\n",
    "# 2. xs, masks and cls classes are all made with seperate calls to train_test_split (in bert_from_scratch.py)\n",
    "#    so maybe the cls' dont match the text\n",
    "\n",
    "num_samples = 10**5 // 5 # 20% of the sample space\n",
    "\n",
    "xs = torch.randint(low=1, high=10, size=(num_samples,5), dtype=torch.int32)\n",
    "xs = torch.cat([torch.zeros(num_samples, 1, dtype=torch.int), xs], dim=1).to(device)\n",
    "\n",
    "ys = xs.clone()\n",
    "cls_label = [float(any(n == 1 for n in xs[i,:])*2-1) for i in range(num_samples)]\n",
    "print(sum(float(any(n == 1 for n in xs[i,:])) for i in range(num_samples))/len(cls_label))\n",
    "\n",
    "split_at = int(0.8 * num_samples)\n",
    "\n",
    "train_x = xs[:split_at, :]\n",
    "train_y = ys[:split_at, :]\n",
    "train_cls = cls_label[:split_at]\n",
    "\n",
    "test_x = xs[split_at:, :]\n",
    "test_y = ys[split_at:, :]\n",
    "test_cls = cls_label[split_at:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f282aab-3987-4a58-98e6-8fe4ffe27259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(torch.Tensor(test_cls) == 1).item() / len(test_cls), sum(torch.Tensor(train_cls) == 1).item() / len(train_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b6699-2264-475b-9a36-0368d7265ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tokentango.BertClassifier(6, 10, device).to(device)\n",
    "#torch.save(model.state_dict(), f\"brute_convergence/{random_id}.pth\")\n",
    "#state_dict = torch.load(\"brute_convergence/did_converge_afcc5594.pth\")\n",
    "#filtered_dict = state_dict #{k: v for k, v in state_dict.items() if not k.startswith('embeddings')}\n",
    "#print(set([n.split(\".\")[0] for n in filtered_dict.keys()]))\n",
    "\n",
    "# Update model's current state_dict\n",
    "#model_state_dict = model.state_dict()\n",
    "#model_state_dict.update(filtered_dict)\n",
    "\n",
    "# Load the updated state_dict into the model\n",
    "#model.load_state_dict(model_state_dict)\n",
    "\n",
    "model.train()\n",
    "# 3. Maybe change named_parameters to parameters? (also in bert_from_scratch.py)\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-4, weight_decay=0.01)\n",
    "#optimizer = Adam(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1140536-20c3-48a4-bba2-a70f3a701541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#body = [*model.encoder_layer.parameters(), *model.transformer_encoder.parameters(), *model.preclassifier.parameters()]\n",
    "#head = [*model.classifier.parameters()]#, model.distribution.parameters()]\n",
    "#embed = [*model.embeddings.parameters()]#, *model.positional_encoding.parameters()]\n",
    "#\n",
    "#hidden_weights = [p for p in body if p.ndim >= 2]\n",
    "#hidden_gains_biases = [p for p in body if p.ndim < 2]\n",
    "#nonhidden_params = [*head, *embed]\n",
    "#\n",
    "#param_groups = [\n",
    "#    dict(params=hidden_weights, use_muon=True,\n",
    "#         lr=0.02, weight_decay=0.01),\n",
    "#    dict(params=hidden_gains_biases+nonhidden_params, use_muon=False,\n",
    "#         lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01),\n",
    "#]\n",
    "#\n",
    "#optimizer = MuonWithAuxAdam(param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2ce3f-f953-4f75-9b09-aa8803779ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "mlm_losses = []\n",
    "cls_losses = []\n",
    "batch_size = 32\n",
    "loss = 1\n",
    "\n",
    "# Initialize variables for tracking best model\n",
    "best_cls_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    epoch_cls_losses = []\n",
    "    \n",
    "    for idx in range(0, split_at//batch_size, batch_size):\n",
    "        #if loss < 0.04:\n",
    "        #    continue\n",
    "        optimizer.zero_grad()\n",
    "        x = train_x[idx:idx+batch_size,:]\n",
    "        y = train_y[idx:idx+batch_size,:]\n",
    "        cls_class = torch.tensor(train_cls[idx:idx+batch_size]).to(device)\n",
    "        hidden = model.hidden(y)\n",
    "        loss_cls = model.classify_loss(hidden, cls_class)\n",
    "        #loss_mlm = model.mlm_loss(hidden, y)\n",
    "        loss = loss_cls# + loss_mlm\n",
    "        \n",
    "        #mlm_losses.append(loss_mlm.cpu().item())\n",
    "        cls_losses.append(loss_cls.cpu().item())\n",
    "        epoch_cls_losses.append(loss_cls.cpu().item())\n",
    "        \n",
    "        loss.backward()\n",
    "        #nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average loss for this epoch\n",
    "    avg_epoch_cls_loss = sum(epoch_cls_losses) / len(epoch_cls_losses)\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if avg_epoch_cls_loss < best_cls_loss:\n",
    "        best_cls_loss = avg_epoch_cls_loss\n",
    "        # Save the best model state (deep copy to avoid reference issues)\n",
    "        best_model_state = {key: value.cpu().clone() for key, value in model.state_dict().items()}\n",
    "        print(f\"New best model at epoch {epoch+1} with cls_loss: {best_cls_loss:.6f}\")\n",
    "\n",
    "# Load the best model weights after training\n",
    "if best_model_state is not None:\n",
    "    # Move state dict back to the appropriate device\n",
    "    for key in best_model_state:\n",
    "        best_model_state[key] = best_model_state[key].to(device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Loaded best model with cls_loss: {best_cls_loss:.6f}\")\n",
    "else:\n",
    "    print(\"No best model found - keeping current weights\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c6167ac-0771-442c-b967-02eaac23885d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "total_super_epochs = 100\n",
    "for super_epoch in range(total_super_epochs):\n",
    "    num_samples = 40_000\n",
    "    \n",
    "    xs = torch.randint(low=1, high=10, size=(num_samples,5), dtype=torch.int32)\n",
    "    xs = torch.cat([torch.zeros(num_samples, 1, dtype=torch.int), xs], dim=1).to(device)\n",
    "    \n",
    "    ys = xs.clone()\n",
    "    cls_label = [float(any(n == 1 for n in xs[i,:]))*2-1 for i in range(num_samples)]\n",
    "    #print(sum(float(any(n == 1 for n in xs[i,:])) for i in range(num_samples))/len(cls_label))\n",
    "    \n",
    "    split_at = int(0.2 * num_samples)\n",
    "    \n",
    "    train_x = xs[:split_at, :]\n",
    "    train_y = ys[:split_at, :]\n",
    "    train_cls = cls_label[:split_at]\n",
    "    \n",
    "    test_x = xs[split_at:, :]\n",
    "    test_y = ys[split_at:, :]\n",
    "    test_cls = cls_label[split_at:]\n",
    "    \n",
    "    model = tokentango.BertClassifier(6, 10, device).to(device)\n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr = 1e-5)\n",
    "    \n",
    "    num_epochs = 32\n",
    "    mlm_losses = []\n",
    "    cls_losses = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        for idx in range(0, int(num_samples/batch_size), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            x = train_x[idx:idx+batch_size,:]\n",
    "            y = train_y[idx:idx+batch_size,:]\n",
    "            cls_class = torch.tensor(train_cls[idx:idx+batch_size]).to(device)\n",
    "    \n",
    "            hidden = model.hidden(x)\n",
    "            loss_cls = model.classify_loss(hidden, cls_class)\n",
    "            #loss_mlm = model.mlm_loss(hidden, y)\n",
    "            loss = loss_cls# + loss_mlm\n",
    "            loss.backward()\n",
    "            #nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #mlm_losses.append(loss_mlm.cpu().item())\n",
    "            cls_losses.append(loss_cls.cpu().item())\n",
    "    \n",
    "    correct = 0\n",
    "    for idx in range(split_at):\n",
    "        x = train_x[idx:idx+1,:]\n",
    "        hidden = model.hidden(x)\n",
    "        output = model.classifier(model.preclassifier(hidden[:,0]).relu())\n",
    "        #print(output.cpu().detach().item(), cls_label[idx:idx+1], x.cpu().detach().tolist())\n",
    "        #correct += int((output.cpu().detach().item()<0.5) == bool(np.signbit(cls_label[idx:idx+1][0])))\n",
    "        correct += int(np.sign(output.cpu().detach().item()) == np.sign(cls_label[idx:idx+1][0]))\n",
    "        #print()\n",
    "    \n",
    "    accuracy = correct / split_at*100\n",
    "\n",
    "    if accuracy > 87:\n",
    "        file_name = f\"did_converge_{super_epoch}.pth\"\n",
    "    else:\n",
    "        file_name = f\"not_converge_{super_epoch}.pth\"\n",
    "\n",
    "    torch.save(model.state_dict(), f\"brute_convergence/{file_name}\")\n",
    "    print(f\"{super_epoch} / {total_super_epochs}...\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c872dc-a8fd-439d-bec1-563fd63ac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(mlm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ee076-e864-47b2-8619-160caad3c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(cls_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c189a-ed4b-4cdf-8e78-0a8f7a946b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "avg_loss = 0\n",
    "for idx in range(num_samples-split_at):\n",
    "    x = test_y[idx:idx+1,:]\n",
    "    hidden = model.hidden(x)\n",
    "    output = model.classify(hidden)\n",
    "    avg_loss += model.classify_loss(hidden, torch.tensor(test_cls[idx:idx+1]).to(device)).cpu().item()\n",
    "    print(f\"{output.cpu().detach().item():.2f} [{test_cls[idx]}] {x.cpu().detach().tolist()}\")\n",
    "    #correct += int((output.cpu().detach().item()<0.5) == (test_cls[idx]<0.5))\n",
    "    correct += int(np.sign(output.cpu().detach().item()) == np.sign(test_cls[idx]))\n",
    "\n",
    "accuracy = correct / (num_samples-split_at)*100\n",
    "avg_loss /= num_samples-split_at\n",
    "print(f'{accuracy}%')\n",
    "\n",
    "#random_id = len([f for f in os.listdir('.') if os.path.isfile(f)])\n",
    "\n",
    "if accuracy > 87:\n",
    "    file_name = f\"did_converge_{random_id}\"\n",
    "else:\n",
    "    file_name = f\"not_converge_{random_id}\"\n",
    "\n",
    "#torch.save(model.state_dict(), f\"brute_convergence/{file_name}.pth\")\n",
    "\n",
    "#fig = px.line(cls_losses)\n",
    "#fig.write_image(f\"brute_convergence/{file_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f743a7-108b-484a-aba9-4727115b6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{file_name} {accuracy}% {avg_loss}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3c7c9dd-8b13-4c51-ad14-e8c58763056f",
   "metadata": {},
   "source": [
    "outputs = []\n",
    "num_samples = len(test_cls)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in range(0, split_at):\n",
    "        x = train_x[idx:idx+1,:]\n",
    "        hidden = model.hidden(x)\n",
    "        output = model.classifier(model.preclassifier(hidden[:,0]).relu())\n",
    "        outputs.append(output)\n",
    "\n",
    "outputs = torch.cat(outputs)\n",
    "\n",
    "predicted_values = torch.round(outputs)\n",
    "predicted_values = predicted_values.cpu().view(-1).numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2c95028-6014-45ac-ba0a-ad751a39f60a",
   "metadata": {},
   "source": [
    "true_values = train_cls\n",
    "label_values = [\"A\", \"B\"]\n",
    "\n",
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(classification_report(true_values, predicted_values, target_names=[str(l) for l in label_values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49aeef2-be7f-4998-8d35-dd3905af6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
