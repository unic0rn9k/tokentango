{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d553d947-1083-461b-91d2-105b65b71bff",
   "metadata": {},
   "source": [
    "# Debug Bert notebook\n",
    "- [x] trivial case, with predicting shifted tokens, instead of mlm.\n",
    "- [x] trivial case, of classifying presence of marker token, at a random position.\n",
    "- [ ] Normalize frequency of labels across training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d79629-e31f-483b-8001-9e8e3362dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTango\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tokentango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe96db1-833e-4ed7-addc-ba792134b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch import nn, functional as F\n",
    "from plotly import express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c577d1e0-1661-4fc0-9023-d380e16a2c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: NVIDIA GeForce GTX 1080\n",
      "  Memory: 8.51 GB\n",
      "  CUDA Capability: 6.1\n",
      "GPU 1: NVIDIA GeForce GTX 1050 Ti\n",
      "  Memory: 4.23 GB\n",
      "  CUDA Capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"  CUDA Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795d072a-db2e-4197-90de-7fb5242be9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f08ba-972d-4825-991c-1cca902e25cf",
   "metadata": {},
   "source": [
    "```py\n",
    "# 1. Embeddings padding_idx = 0, might conflict with cls, or other tokens? (model.py)\n",
    "# 2. xs, masks and cls classes are all made with seperate calls to train_test_split (in bert_from_scratch.py)\n",
    "#    so maybe the cls' dont match the text\n",
    "num_samples = 2000\n",
    "\n",
    "xs = torch.randint(low=1, high=10, size=(num_samples,5), dtype=torch.int32)\n",
    "xs = torch.cat([torch.zeros(num_samples, 1, dtype=torch.int), xs], dim=1).to(device)\n",
    "\n",
    "ys = xs.clone()\n",
    "cls_label = [float(any(n == 1 for n in xs[i,:])) for i in range(num_samples)]\n",
    "\n",
    "split_at = int(0.2 * num_samples)\n",
    "\n",
    "train_x = xs[:split_at, :]\n",
    "train_y = ys[:split_at, :]\n",
    "train_cls = cls_label[:split_at]\n",
    "\n",
    "test_x = xs[split_at:, :]\n",
    "test_y = ys[split_at:, :]\n",
    "test_cls = cls_label[split_at:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e94ac0e-b81b-48fb-8e2b-0e14fd1ea092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigbox/Projects/tokentango/src/tokentango/fake_news.py:15: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  large_set = pd.read_csv(\"995,000_rows.csv\").sample(frac=frac).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "new_labels\n",
      "reliable    371787\n",
      "fake        186982\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigbox/Projects/tokentango/src/tokentango/fake_news.py:63: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  large_set = large_set.groupby('new_labels').apply(lambda x: x.sample(min_count, random_state=42)).sample(frac=1).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#train_x = tokentango.fake_news.train_mlm\n",
    "#train_y = tokentango.fake_news.train_x\n",
    "#train_cls = tokentango.fake_news.train_y\n",
    "train_x, train_y, train_cls, test_x, test_y, test_cls = tokentango.fake_news.load_data(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778d48e6-ba46-4250-bc04-d05be437b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3578/1743132160.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_y = torch.tensor(test_y).to(device)\n",
      "/tmp/ipykernel_3578/1743132160.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_cls = torch.tensor(test_cls).to(device)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x.to(device)\n",
    "train_y = train_y.to(device)\n",
    "train_cls = train_cls.to(device)\n",
    "\n",
    "test_x = torch.tensor(test_x).to(device)\n",
    "test_y = torch.tensor(test_y).to(device)\n",
    "test_cls = torch.tensor(test_cls).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c81cf9-22f7-4d76-be83-a5f2dd1882b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size: 1.6731 GB\n"
     ]
    }
   ],
   "source": [
    "size_in_bytes = train_x.element_size() * train_x.numel() + train_y.element_size() * train_y.numel() + train_cls.element_size() * train_cls.numel()\n",
    "size_in_bytes2 = test_x.element_size() * test_x.numel() + test_y.element_size() * test_y.numel() + test_cls.element_size() * test_cls.numel()\n",
    "size_in_gb = (size_in_bytes + size_in_bytes2) / (1024 ** 3)\n",
    "\n",
    "print(f\"Tensor size: {size_in_gb:.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1257b0-9e78-4b61-9b11-a1724d0250aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6748380661010742 GB allocated\n",
      "1.677734375 GB reserved by caching allocator\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.memory_reserved() / \u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGB reserved by caching allocator\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Optional: summary of tensors by type\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchsummary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Or more advanced: use torch.cuda.memory_summary\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.memory_summary(device=\u001b[38;5;28;01mNone\u001b[39;00m, abbreviated=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "# Current memory usage\n",
    "print(torch.cuda.memory_allocated() / 1024**3, \"GB allocated\")\n",
    "print(torch.cuda.memory_reserved() / 1024**3, \"GB reserved by caching allocator\")\n",
    "\n",
    "# Optional: summary of tensors by type\n",
    "from torchsummary import summary\n",
    "\n",
    "# Or more advanced: use torch.cuda.memory_summary\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805b6699-2264-475b-9a36-0368d7265ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tokentango.BertClassifier(300, 40000, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f2ce3f-f953-4f75-9b09-aa8803779ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigbox/Projects/tokentango/src/tokentango/train.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  import datetime as dt\n",
      "/home/bigbox/Projects/tokentango/src/tokentango/train.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  random_offset = random.randint(0, len(test_cls) - sample_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 ms, sys: 20 ms, total: 119 ms\n",
      "Wall time: 149 ms\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 730.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 689.31 MiB is free. Including non-PyTorch memory, this process has 7.23 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 144.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# 1. embedding padding_idx (from model.py) should probably be set to pad_token_id (from fake_news.py)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# 3. Maybe change named_parameters to parameters? (also in bert_from_scratch.py)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#optimizer = AdamW(model.parameters(), lr = 1e-4, eps = 1e-8)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mresult = tokentango.train.train(model, train_x, train_y, train_cls, test_x, test_y, test_cls, device)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tokentango/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tokentango/.venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tokentango/.venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:5\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tokentango/src/tokentango/train.py:21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_x, train_y, train_cls, test_x, test_y, test_cls, device)\u001b[39m\n\u001b[32m     18\u001b[39m sample_size = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_cls) * \u001b[32m0.1\u001b[39m))\n\u001b[32m     19\u001b[39m random_offset = random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_cls) - sample_size)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(random_offset, random_offset + sample_size):\n\u001b[32m     23\u001b[39m     x = test_y[idx:idx+\u001b[32m1\u001b[39m,:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tokentango/src/tokentango/model.py:58\u001b[39m, in \u001b[36mBertClassifier.mlm_loss\u001b[39m\u001b[34m(self, hidden, mb_x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmlm_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden, mb_x):\n\u001b[32m     57\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.distribution(hidden)[:,\u001b[32m1\u001b[39m:,:]\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     loss_mlm = torch.nn.functional.cross_entropy(\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m, mb_x[:,\u001b[32m1\u001b[39m:].reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mlm\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 730.00 MiB. GPU 0 has a total capacity of 7.92 GiB of which 689.31 MiB is free. Including non-PyTorch memory, this process has 7.23 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 144.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. embedding padding_idx (from model.py) should probably be set to pad_token_id (from fake_news.py)\n",
    "# 3. Maybe change named_parameters to parameters? (also in bert_from_scratch.py)\n",
    "#optimizer = AdamW(model.parameters(), lr = 1e-4, eps = 1e-8)\n",
    "\n",
    "result = tokentango.train.train(model, train_x, train_y, train_cls, test_x, test_y, test_cls, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c872dc-a8fd-439d-bec1-563fd63ac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(mlm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ee076-e864-47b2-8619-160caad3c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(cls_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d56c6-55a2-4f9a-9a50-0d2ff75fbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea8886-d0c7-4a10-9349-33ba0c874b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for idx in range(len(train_cls)):\n",
    "    x = train_y[idx:idx+1,:]\n",
    "    hidden = model.hidden(x)\n",
    "    output = model.classify(hidden)\n",
    "    #print(output.cpu().detach().item(), cls_label[idx:idx+1], x.cpu().detach().tolist())\n",
    "    #correct += int((output.cpu().detach().item()<0.5) == bool(np.signbit(cls_label[idx:idx+1][0])))\n",
    "    correct += int(np.sign(output.cpu().detach().item()) == np.sign(train_cls[idx].cpu()))\n",
    "\n",
    "accuracy = correct / len(test_cls)*100\n",
    "print(f'{accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04c45b-867d-46da-9500-7fef86dc3387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "num_samples = len(test_cls)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in range(0, num_samples):\n",
    "        x = train_x[idx:idx+1,:]\n",
    "        hidden = model.hidden(x)\n",
    "        output = model.classify(hidden)\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff53e82-e900-4912-8959-8ae1abc1fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.sign(torch.cat(outputs))\n",
    "\n",
    "predicted_values = torch.round(outputs)\n",
    "predicted_values = predicted_values.cpu().view(-1).numpy()\n",
    "true_values = test_cls.cpu().numpy()\n",
    "label_values = [\"reliabel\", \"fake\"]\n",
    "\n",
    "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
    "print (\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(classification_report(true_values, predicted_values, target_names=[str(l) for l in label_values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438ba9f-17a2-45f0-b618-8e9df4784e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "cm_test = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#plt.figure(figsize=(6,6))\n",
    "#plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset')\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cm_test, classes=label_values, title='Confusion Matrix - Test Dataset', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977098d-9a4c-40b2-8282-166b722aa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(n == 1 for n in predicted_values))\n",
    "print(sum(n == -1 for n in predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb959193-3b5f-4434-b4de-0b13e0be2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(n == 1 for n in train_cls))\n",
    "print(sum(n == -1 for n in train_cls))\n",
    "print(sum(n == 1 for n in test_cls))\n",
    "print(sum(n == -1 for n in test_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691421e3-886d-4820-b01e-ff153225c43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67853909-a5be-46a6-8458-9cac749bcb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d40c4-270f-48ea-85f6-f699856e405d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
